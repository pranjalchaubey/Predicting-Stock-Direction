{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stock-Direction.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pranjalchaubey/Predicting-Stock-Direction/blob/master/src/Stock_Direction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrA9m16Wv5Hq",
        "colab_type": "text"
      },
      "source": [
        "# Stock Direction\n",
        "In this notebook we are going to use Random Forests to predict the direction  \n",
        "of stocks for the next trading day.  \n",
        "In more technical terms, we are going to try and predict the _**Momentum**_  \n",
        "of the stocks.  \n",
        "Based on the momentum, we are going to give the following signals to the user  \n",
        "1. Buy (_+ve momentum_)\n",
        "2. Sell (_-ve momentum_)\n",
        "3. Hold (_neutral_)  \n",
        "\n",
        "This system will work on the daily closing prices and will generate the signals by predicting the momentum for the next trading day. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rm2r2nZLyPpP",
        "colab_type": "text"
      },
      "source": [
        "##Setup The Colab Environment \n",
        "Install and Import the required libraries.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHPgN_JWRont",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Debug Flag\n",
        "_GLOBAL_DEBUG_ = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPIEFSuJz3_Z",
        "colab_type": "code",
        "outputId": "9415f8a9-020f-4f5f-f435-f65155d68feb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Install the yFinance library \n",
        "!pip install -q yfinance "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for yfinance (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHORMgl6zxqJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# yFinance will help us fetch the data for our dataset\n",
        "import yfinance as yf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALAkdXJVv0IL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data Mining and plotting \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70KcLBXhy08M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Classifier \n",
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujJcQHQTzV2a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Other Libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from tqdm import tqdm\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "# if in Google Colaboratory\n",
        "try:\n",
        "    from google.colab import drive\n",
        "except:\n",
        "    pass\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eXZSuMUzZ6I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Configure Pandas Display Options\n",
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.width', 1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMqMKQ6pzc8o",
        "colab_type": "code",
        "outputId": "f173d1ca-a3ee-4176-f56c-b4df62c414eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "source": [
        "# Since I am running this notebook on Colab, \n",
        "# Let's try and get some system information \n",
        "import platform\n",
        "print('System Processor: ', platform.processor(), '\\n')\n",
        "!nvidia-smi"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "System Processor:  x86_64 \n",
            "\n",
            "Wed Oct  2 22:16:59 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 430.40       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P8    25W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDkAce5w_LT4",
        "colab_type": "text"
      },
      "source": [
        "## Load the Data\n",
        "We will extract the 5 year history of 10 stocks chosen by the user. \n",
        "\n",
        "<br/>Here, we will take the following stocks as an example\n",
        "1. Facebook FB\n",
        "2. Apple AAPL\n",
        "3. Amazon AMZN\n",
        "4. Netflix NFLX\n",
        "5. Google GOOGL\n",
        "6. Starbucks SBUX\n",
        "7. Exxon Mobil XOM\n",
        "8. Johnson & Johnson JNJ\n",
        "9. Bank of America BAC\n",
        "10. General Motors GM\n",
        "\n",
        "<br/>In the actual application, stock selection will be done by the user. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MO3eCg6TzeeR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# List of stock tickers \n",
        "# this info will come from the user, perhaps in the form of a pickle file \n",
        "ticker_list = ['FB', 'AAPL', 'AMZN', 'NFLX', 'GOOGL', 'SBUX', 'XOM', 'JNJ',\\\n",
        "               'BAC', 'GM']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nk95CIFIW9H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fetch_ticker_data(tickers): \n",
        "    \"\"\"\n",
        "    Fetch 5 years of historical data for the 'tickers', from Yahoo Finance.    \n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    tickers : List   \n",
        "        List of stocks chosen by the user.   \n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    data : Dataframe \n",
        "        Pandas df with historical stock data time series. \n",
        "    \"\"\"\n",
        "    # Debug \n",
        "    _LOCAL_DEBUG_ = False\n",
        "\n",
        "    # We are going to get 10 years worth of stock data\n",
        "    # Generate the required timestamps \n",
        "    # t_now = pd.datetime.now().date() - pd.DateOffset(n=1)\n",
        "    t_now = pd.datetime.now().date()\n",
        "    t_10_year = (t_now - pd.DateOffset(n=3650)).date()\n",
        "    if _GLOBAL_DEBUG_ and _LOCAL_DEBUG_:\n",
        "        print(t_now)\n",
        "        print(t_10_year)\n",
        "\n",
        "    # Get the Data from Yahoo! Finance\n",
        "    data = yf.download(tickers, start=t_10_year, end=t_now)\n",
        "    if _GLOBAL_DEBUG_ and _LOCAL_DEBUG_:\n",
        "        display(data.head(20))\n",
        "\n",
        "    # Data is multi-indexed on the columns \n",
        "    # Make it multi-index on the rows, to make it \n",
        "    # fit for consumption by the RandomForestClassiffier\n",
        "    data = data.stack(1) \n",
        "    if _GLOBAL_DEBUG_ and _LOCAL_DEBUG_:\n",
        "        display(data.head(20))\n",
        "\n",
        "    # Drop all the columns except for Adj Close & Volume\n",
        "    data = data[['Adj Close', 'Volume']]\n",
        "    # Rename the column names \n",
        "    data.columns = ['close', 'volume']  \n",
        "    if _GLOBAL_DEBUG_ and _LOCAL_DEBUG_:\n",
        "        display(data.head(20))\n",
        "\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPtLBV75SD08",
        "colab_type": "code",
        "outputId": "ba580281-5719-417d-81be-26b05afdc0e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Go fetch! \n",
        "dataset = fetch_ticker_data(tickers=ticker_list)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*********************100%***********************]  10 of 10 downloaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lwSmUWMplIb",
        "colab_type": "text"
      },
      "source": [
        "## Data Preprocessing\n",
        "Now that we have the data downloaded and neatly organized in a dataframe,  \n",
        "time to do some necessary preprocessing.  \n",
        "In particular, we are going to use the _log returns_ of the stocks to create a  \n",
        "target column, called `target`.  \n",
        "The target variable will be created using the following criteria  \n",
        "```\n",
        "  -1 = Sell = ret < -0.0015\n",
        "   0 = Hold = -0.0015 < ret < 0.0015 \n",
        "   1 = Buy  = ret > .0015\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4H28uqBYSjT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def classify_return(ret):\n",
        "    \"\"\"\n",
        "    Classify the returns as -1, 0 or 1.\n",
        "    -1 = Sell = ret < -0.0015\n",
        "    0 = Hold = -0.0015 < ret < 0.0015 \n",
        "    1 = Buy  = ret > .0015\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    ret : Float   \n",
        "        Stock return for the current day, d. \n",
        "        retd = log(retd) - log(ret(d-1))\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    ret_category : Integer\n",
        "        Category of return, ret. \n",
        "        -1, 0 or 1. \n",
        "    \"\"\"\n",
        "    ret_category = 0\n",
        "    \n",
        "    if ret < -0.0015: \n",
        "        ret_category = -1\n",
        "    elif -0.0015 < ret and ret < 0.0015:\n",
        "        ret_category = 0\n",
        "    elif ret > 0.0015:\n",
        "        ret_category = 1\n",
        "\n",
        "    return ret_category"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofeAWTqHr1tv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_preprocess(dataset):\n",
        "    \"\"\"\n",
        "    Calculates log returns and creates the 'target' column.\n",
        "    Returns the processed dataset as well as the data for the current \n",
        "    date separately on which the prediction has to be performed. \n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    dataset : Pandas Dataframe\n",
        "        Dataframe containing price, volume and other \n",
        "        information of the chosen stocks. \n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dataset: DataFrame\n",
        "        Processed dataframe with two additional columns,\n",
        "        'returns' and 'target'. \n",
        "        returns = log returns of prices \n",
        "        target = target column for the classifier to predict\n",
        "    pred_data: DataFrame\n",
        "        Data for the 'current date' on which the trained classifier \n",
        "        will perform the prediction. The classifier output from \n",
        "        this prediction will be visible to the user. \n",
        "    \"\"\"\n",
        "    # Debug Flag\n",
        "    _LOCAL_DEBUG_ = False\n",
        "\n",
        "    # Advance the prices by 1 day to calculate the log returns \n",
        "    dataset['shift'] = dataset.groupby(level=1)['close'].shift(1)\n",
        "    if _GLOBAL_DEBUG_ and _LOCAL_DEBUG_:\n",
        "        display(dataset.head(30))\n",
        "\n",
        "    # Create the 'returns' column\n",
        "    dataset['returns'] = np.log(dataset['close']) - np.log(dataset['shift'])\n",
        "    if _GLOBAL_DEBUG_ and _LOCAL_DEBUG_:\n",
        "        #print('Return Head')\n",
        "        #display(dataset.head(30))\n",
        "        print('Return Tail')\n",
        "        display(dataset.tail(30))\n",
        "\n",
        "    # Drop the first row as it contains NANs in the returns column\n",
        "    dataset.drop(index = dataset.index.levels[0].values[0], level=0,\\\n",
        "                 inplace=True)\n",
        "    if _GLOBAL_DEBUG_ and _LOCAL_DEBUG_:\n",
        "        display(dataset.head(30))\n",
        "\n",
        "    # Create the 'target' column for the classifier \n",
        "    # Shift the 'target' into the future/delay by 1 day 'shift(-1)'\n",
        "    # We do this since we are predicting returns of the 'next day close'\n",
        "    dataset['target'] = dataset['returns'].apply(lambda x: classify_return(x))\n",
        "    dataset['target'] = dataset.groupby(level=1)['target'].shift(-1)\n",
        "    if _GLOBAL_DEBUG_ and _LOCAL_DEBUG_:\n",
        "        #print('Target Head')\n",
        "        #display(dataset.head(30))\n",
        "        print('Target Tail')\n",
        "        display(dataset.tail(30))\n",
        "\n",
        "    # Drop the 'shift' column as it was only temporary to calculate 'returns'\n",
        "    dataset.drop(['shift'], axis=1, inplace=True)\n",
        "    if _GLOBAL_DEBUG_ and _LOCAL_DEBUG_:\n",
        "        display(dataset.head(30))\n",
        "\n",
        "    # Extract the data for the current date, before dropping NANs \n",
        "    pred_data = dataset.iloc[-(len(ticker_list)):]\n",
        "    if _GLOBAL_DEBUG_ and _LOCAL_DEBUG_:\n",
        "        print('pred_data')\n",
        "        display(pred_data)\n",
        "\n",
        "    # Handle NANs\n",
        "    # We keep it simple at the moment and simply drop off rows with NANs \n",
        "    if _GLOBAL_DEBUG_ and _LOCAL_DEBUG_:\n",
        "        print(dataset.shape)\n",
        "        print(dataset.isna().sum())\n",
        "    dataset.dropna(how='any', inplace=True)\n",
        "    if _GLOBAL_DEBUG_ and _LOCAL_DEBUG_:\n",
        "        print(dataset.shape)\n",
        "        print(dataset.isna().sum())\n",
        "        print('Final Dataset Tail')\n",
        "        display(dataset.tail(30))\n",
        "\n",
        "    return dataset, pred_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zi4Q9kt2zh7a",
        "colab_type": "text"
      },
      "source": [
        "Pre-process the downloaded dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVvKp_pvzBxz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if _GLOBAL_DEBUG_:\n",
        "    display(dataset.head(20))\n",
        "dataset, pred_data = data_preprocess(dataset)\n",
        "if _GLOBAL_DEBUG_:\n",
        "    display(dataset.head(20))\n",
        "    display(dataset.tail())\n",
        "    display(pred_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fADHPJVg4XpP",
        "colab_type": "text"
      },
      "source": [
        "## Feature Engineering \n",
        "Leaving this section blank at the moment. Will come back at a later time. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Vs3FeQ5CGBo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cET3Gtd47bcf",
        "colab_type": "text"
      },
      "source": [
        "## Train Test Split \n",
        "Split the data into testing and training sets.  \n",
        "In production, there's going to be only re-training of the model that we have  \n",
        "chosen with the specified hyper-parameters. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0eNFIrW-Uao",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_test_split(dataset, features, target, train_size, test_size):\n",
        "    \"\"\"\n",
        "    Generate the train and test dataset.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    dataset : DataFrame\n",
        "        All the samples including target\n",
        "    features : List\n",
        "        List of the names of columns that are features\n",
        "    target : String\n",
        "        Name of column that is the target (in our case, 'target')\n",
        "    train_size : float\n",
        "        The proportion of the data used for the training dataset\n",
        "    test_size : float\n",
        "        The proportion of the data used for the test dataset\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    x_train : DataFrame\n",
        "        The train input samples\n",
        "    x_test : DataFrame\n",
        "        The test input samples\n",
        "    y_train : Pandas Series\n",
        "        The train target values\n",
        "    y_test : Pandas Series\n",
        "        The test target values\n",
        "    \"\"\"\n",
        "    # Data Sanity check \n",
        "    assert train_size >= 0 and train_size <= 1.0, 'Train size out of bounds!'\n",
        "    assert test_size >= 0 and test_size <= 1.0, 'Test size out of bounds!'\n",
        "    assert train_size + test_size == 1.0, 'Train + Test should be equal to 1!'\n",
        "    \n",
        "    # Debug Flag \n",
        "    _LOCAL_DEBUG_ = True \n",
        "    \n",
        "    # Extract the x and y from the dataset\n",
        "    all_x = dataset[features]\n",
        "    all_y = dataset[target]\n",
        "    if _GLOBAL_DEBUG_ and _LOCAL_DEBUG_:\n",
        "        print(all_x.head())\n",
        "        print(all_y.head())\n",
        "        print('dataset.shape ', dataset.shape)\n",
        "\n",
        "    # Get the number of rows in df and no of elements in the pandas series \n",
        "    # NOTE - Both are multi-indexed\n",
        "    len_x = len(all_x.index.levels[0])\n",
        "    len_y = len(all_y.index.levels[0])\n",
        "    if _GLOBAL_DEBUG_ and _LOCAL_DEBUG_:\n",
        "        print('Len x ', len_x)\n",
        "        print('Len y ', len_y)\n",
        "    \n",
        "    # Some fancy calculations here for x\n",
        "    x_train = all_x.loc[all_x.index.levels[0]\\\n",
        "                        [:int(len_x*train_size)].astype(str).tolist()]\n",
        "    x_test = all_x.loc[all_x.index.levels[0]\\\n",
        "                       [int(len_x*train_size):int(len_x*train_size) + int(len_x*test_size)]\\\n",
        "                       .astype(str).tolist()]\n",
        "    if _GLOBAL_DEBUG_ and _LOCAL_DEBUG_:\n",
        "        print('x_train.shape ', x_train.shape)\n",
        "        print('x_test.shape ', x_test.shape)\n",
        "\n",
        "    # Some fancy calculations here for y as well\n",
        "    y_train = all_y.loc[all_y.index.levels[0]\\\n",
        "                        [:int(len_y*train_size)].astype(str).tolist()]\n",
        "    y_test = all_y.loc[all_y.index.levels[0]\\\n",
        "                       [int(len_y*train_size):int(len_y*train_size) + int(len_y*test_size)]\\\n",
        "                       .astype(str).tolist()]\n",
        "    if _GLOBAL_DEBUG_ and _LOCAL_DEBUG_:\n",
        "        print('y_train.shape ', y_train.shape)\n",
        "        print('y_test.shape ', y_test.shape)\n",
        " \n",
        "    return x_train, x_test, y_train, y_test "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rrKKH3lKfN6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# In our dataset all the columns except for the 'target' column are features\n",
        "# Temporarily drop the 'target' and extract the features  \n",
        "features = dataset.drop(['target'], axis=1).columns.values.tolist()\n",
        "\n",
        "# Get the train and test sets \n",
        "X_train, X_test, y_train, y_test = train_test_split(dataset=dataset,\\\n",
        "                                                    features=features,\\\n",
        "                                                    target='target',\\\n",
        "                                                    train_size=0.9,\\\n",
        "                                                    test_size=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1MpA89NIlXG",
        "colab_type": "text"
      },
      "source": [
        "## Hyperparameter Search\n",
        "We have the dataset in our hand with train and test splits.  \n",
        "Time to train our _Random Forest Classifier_, and look for the best set of  \n",
        "hyperparameters.  \n",
        "\n",
        "1. Create a plotting function to help in evaluating the performance of RFCs  \n",
        "2. Display the ranked feature importances \n",
        "3. Declare a hyperparameter _'grid'_ for the various RFCs to train on\n",
        "4. Train the RFCs \n",
        "5. Based on performance, choose the best set of hyperparameters. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNBC0I47LHG1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot(xs, ys, labels, title='', x_label='', y_label=''):\n",
        "    \"\"\"\n",
        "    Generate a plot of xs vs ys.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    xs : List\n",
        "        List of x-axis values  \n",
        "    ys : List\n",
        "        List of y-axis values  \n",
        "    labels : List\n",
        "        List of strings containing the name of legends\n",
        "    title : String\n",
        "        Plot Title\n",
        "    x_label : String\n",
        "        Name of the x-axis\n",
        "    y_label : String\n",
        "        Name of the y-axis\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None. Just plots the plot.\n",
        "    \"\"\"\n",
        "    # Set the figure size\n",
        "    plt.rcParams['figure.figsize'] = [12, 6]\n",
        "\n",
        "    # Build the plots\n",
        "    for x, y, label in zip(xs, ys, labels):\n",
        "        plt.ylim((0.4, 0.7))\n",
        "        plt.plot(x, y, label=label)\n",
        "\n",
        "    # Set the titles and lables \n",
        "    plt.title(title)\n",
        "    plt.xlabel(x_label)\n",
        "    plt.ylabel(y_label)\n",
        "\n",
        "    # Align the legends and show the plot \n",
        "    plt.legend(bbox_to_anchor=(1.04, 1), borderaxespad=0)\n",
        "    plt.show()\n",
        "\n",
        "    return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o905aKn-McnA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rank_features_by_importance(importances, feature_names):\n",
        "    \"\"\"\n",
        "    Ranks and Pretty prints the feature importances as \n",
        "    evaluated by the Random Forest Classifier. \n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    importances : ndarray\n",
        "        A numpy array containing feature importance values  \n",
        "    feature_names : List\n",
        "        A list of strings containing the names of features\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None. Just ranks and pretty prints the features along \n",
        "    with their importance. \n",
        "    \"\"\"\n",
        "    # Get the sorted INDEX of the importances array \n",
        "    indices = np.argsort(importances)[::-1]\n",
        "    # Use this parameter to align the print output\n",
        "    max_feature_name_length = max([len(feature) for feature in feature_names])\n",
        "\n",
        "    # Pretty Print the ranked features and their respective importances \n",
        "    for x_train_i in range(len(importances)):\n",
        "        print('{number:>2}. {feature: <{padding}} ({importance})'.format(\n",
        "            number=x_train_i + 1,\n",
        "            padding=max_feature_name_length,\n",
        "            feature=feature_names[indices[x_train_i]],\n",
        "            importance=importances[indices[x_train_i]]))\n",
        "\n",
        "    return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLUsSjAmOfR9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyperparameters \n",
        "# This is to get consistent results between each run.\n",
        "clf_random_state = 0\n",
        "\n",
        "n_days = 5\n",
        "n_stocks = len(ticker_list)\n",
        "\n",
        "clf_parameters = {\n",
        "    'criterion': 'entropy',\n",
        "    'min_samples_leaf': n_stocks * n_days,\n",
        "    'oob_score': True,\n",
        "    'n_jobs': -1,\n",
        "    'random_state': clf_random_state}\n",
        "n_trees_l = [5, 10, 25, 50, 75, 100]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V86wQtriJ0LB",
        "colab_type": "code",
        "outputId": "e70e8362-55ca-4045-83e9-2197b12bd687",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# %%timeit -n 1 -r 1\n",
        "# Train the Classifier \n",
        "train_score = []\n",
        "test_score = []\n",
        "feature_importances = []\n",
        "\n",
        "for n_trees in tqdm(n_trees_l, desc='Training Models', unit='Model'):\n",
        "    # Train \n",
        "    clf = RandomForestClassifier(n_trees, **clf_parameters)\n",
        "    clf.fit(X_train, y_train)\n",
        "    # Performance Parameters \n",
        "    train_score.append(clf.score(X_train, y_train.values))\n",
        "    test_score.append(clf.score(X_test, y_test.values))\n",
        "    feature_importances.append(clf.feature_importances_)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Models: 100%|██████████| 6/6 [00:11<00:00,  2.47s/Model]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBguIFTONDUu",
        "colab_type": "code",
        "outputId": "f2027932-e3ff-4884-8178-754d9952dc5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 491
        }
      },
      "source": [
        "# The most important features \n",
        "print('Features Ranked by Average Importance:\\n')\n",
        "rank_features_by_importance(np.average(feature_importances, axis=0), features)\n",
        "\n",
        "# Visualize the performance of the various classifiers \n",
        "plot([n_trees_l]*3,\n",
        "    [train_score, test_score],\n",
        "    ['train', 'test'],\n",
        "    'Random Forrest Accuracy',\n",
        "    'Number of Trees')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Features Ranked by Average Importance:\n",
            "\n",
            " 1. close   (0.34068413459776337)\n",
            " 2. returns (0.33720547144341717)\n",
            " 3. volume  (0.32211039395881946)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuQAAAGDCAYAAABjvQUaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAJDQ/wdtp+RR5nzW9QAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKLfKrAlFvf-",
        "colab_type": "text"
      },
      "source": [
        "## Train the Best Model \n",
        "From the above set of Random Forests Classifiers, we can observe that the RFC  \n",
        "with 25 trees has performed the best.  \n",
        "Notice how the performance on the Testing Set _bumps up_ for the RFC with 25  \n",
        "tress, and then continues going down for RFCs having higher number of trees.  \n",
        "Let's train this model (again) and use it for predictions.  \n",
        "<br/>\n",
        "_**However, there's not going to be any test set this time. We will use all  \n",
        "the data to train our model and make a prediction for the next trading day.**_  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9PW6wHS6loz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the train and test sets \n",
        "X_train, X_test, y_train, y_test = train_test_split(dataset=dataset,\\\n",
        "                                                    features=features,\\\n",
        "                                                    target='target',\\\n",
        "                                                    train_size=1.0,\\\n",
        "                                                    test_size=0.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAhMaPGfIPQu",
        "colab_type": "code",
        "outputId": "eb2dddc6-7bde-49cc-a23a-554577f5fd8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Train the Random Forest Classifier on this data \n",
        "# We have selected n_trees to be 25\n",
        "n_trees = 25\n",
        "clf = RandomForestClassifier(n_trees, **clf_parameters)\n",
        "clf.fit(X_train, y_train)\n",
        "# Get the Training score (just for us)\n",
        "print(f'Classifier Accuracy: {clf.score(X_train, y_train.values):0.3f}')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classifier Accuracy: 0.552\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tc9g_2H1c6Pn",
        "colab_type": "text"
      },
      "source": [
        "## Make the Prediction\n",
        "Now that we have trained our model with the best set of hyperparameters, time  \n",
        "to make the prediction for the next date (that is tomorrow's close).  \n",
        "The output from this prediction will be shown to the user. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQsSA6Hqda78",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_prediction(pred_data, features, rfc_classifier):\n",
        "    \"\"\"\n",
        "    Makes a price prediction on the stocks as selected by the user. \n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    pred_data : Dataframe\n",
        "        This dataframe contains only 1 date, today's date (or yesterday's if \n",
        "        the markets haven't close yet). The data on this date will be used to \n",
        "        make a prediction about the closing prices for next day/tomorrow.\n",
        "    feature : List\n",
        "        List of features present in pred_data. It also has a 'target' column, \n",
        "        we wont be using it. \n",
        "    rfc_classifier : Random Forest Classifier Object \n",
        "        Trained Random forest classifier with the best hyperparameters  \n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pred_list: List \n",
        "        List containing tuples in the following format, \n",
        "        [(Stock1, Pred1), (Stock2, Pred2),........]\n",
        "    \"\"\"\n",
        "    # Make the prediction \n",
        "    prediction = rfc_classifier.predict(pred_data[features])\n",
        "    # Create the 'Prediction List' \n",
        "    pred_list = [(stock, pred) for stock, pred in\\\n",
        "                 zip(pred_data.index.levels[1].values.tolist(), prediction)]\n",
        "\n",
        "    return pred_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StmaXuJ0h8WR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_prediction = make_prediction(pred_data=pred_data, features=features,\\\n",
        "                                   rfc_classifier=clf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygfhh3WjiY2f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "290a9422-1aca-4dd1-880b-e4bccf336968"
      },
      "source": [
        "final_prediction"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('AAPL', 1.0),\n",
              " ('AMZN', 1.0),\n",
              " ('BAC', 1.0),\n",
              " ('FB', 1.0),\n",
              " ('GM', 1.0),\n",
              " ('GOOGL', -1.0),\n",
              " ('JNJ', 1.0),\n",
              " ('NFLX', 1.0),\n",
              " ('SBUX', 1.0),\n",
              " ('XOM', 1.0)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    }
  ]
}